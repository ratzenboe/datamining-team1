{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from scipy.sparse import csr_matrix\n",
    "import copy\n",
    "\n",
    "\n",
    "class PreDeCon:\n",
    "    def __init__(self, data, epsilon, delta, mu, lamb, kappa):\n",
    "        \"\"\"\n",
    "        :param data: data array: numpy ndarray\n",
    "        :param epsilon: Defines region of interest for each point in data set; radius parameter for variance calcs\n",
    "        :param delta: Threshold parameter for variance along attriubte determining the subspace preference dim.\n",
    "        :param mu: Minimum number of points (similar to DBSCAN)\n",
    "        :param lamb: maximum subspace preference dimensionaliy to be considered a core point\n",
    "        :param kappa: subspace scaling factor; usually k>>1\n",
    "        \"\"\"\n",
    "        # data and hyperparameters.\n",
    "        self.data = data\n",
    "        self.epsilon = epsilon\n",
    "        self.delta = delta\n",
    "        self.mu = mu\n",
    "        self.lamb = lamb\n",
    "        self.kappa = kappa\n",
    "\n",
    "        # Number of points and dimensions\n",
    "        self.nb_points, self.nb_dimensions = data.shape\n",
    "\n",
    "        # Nearest neighbor structure\n",
    "        self.neigbors_clf = None\n",
    "            \n",
    "        # Fill member variables at initialization\n",
    "        self.epsilon_neighbourhoods = self.create_epsilon_neighbourhoods()\n",
    "        self.attribute_variances = self.create_variances_along_attributes()\n",
    "        self.subspace_preference_dimensionalities = self.create_subspace_preference_dimensionality()\n",
    "        self.subspace_preference_vectors = self.create_subspace_preference_vectors()\n",
    "        self.pref_weighted_similarity_measures = self.create_preference_weighted_similarity_matrix()\n",
    "        self.preference_weighted_neighbourhoods = self.create_preference_weighted_epsilon_neighbourhood()\n",
    "        self.preference_weighted_core_points = self.create_preference_weighted_core_points()\n",
    "\n",
    "        # Cluster label information\n",
    "        self.labels_ = np.full(shape=(self.nb_points,), fill_value=np.nan)\n",
    "\n",
    "    def create_epsilon_neighbourhoods(self):\n",
    "        \"\"\"Calculate epsilon-neighbourhood for each data point\n",
    "        :return: numpy ndarray where each row contains the indices of points that\n",
    "                 are in the neighborhood of a certain point\n",
    "        \"\"\"\n",
    "        self.neigbors_clf = NearestNeighbors(radius=self.epsilon, algorithm='ball_tree')\n",
    "        self.neigbors_clf.fit(self.data)\n",
    "        _, neigh_idx = self.neigbors_clf.radius_neighbors(self.data)\n",
    "        return neigh_idx\n",
    "\n",
    "    def create_variances_along_attributes(self):\n",
    "        \"\"\"Calculate variance across attributes\n",
    "        :return: numpy ndarray: rows=data points, columns= variance in corresponding attribute\n",
    "        \"\"\"\n",
    "        attribute_vars = np.empty(shape=(0, self.nb_dimensions))\n",
    "        for idx in range(self.nb_points):\n",
    "            # Calculate attribute variances\n",
    "            var_attr = np.sum((self.data[idx] - self.data[self.epsilon_neighbourhoods[idx]]) ** 2, axis=0) / len(\n",
    "                self.epsilon_neighbourhoods[idx])\n",
    "            attribute_vars = np.vstack([attribute_vars, var_attr])  # Save them in numpy ndarray\n",
    "        return attribute_vars\n",
    "\n",
    "    def create_subspace_preference_dimensionality(self):\n",
    "        \"\"\"Calcuate subspace preference dimensionaliy from the variance across attributes\n",
    "        :return: numpy 1D array: one dimensionality value for each data point\n",
    "        \"\"\"\n",
    "        # For each point compute number of dimensions that have a lower variance then delta\n",
    "        spd = np.count_nonzero(self.attribute_variances < self.delta, axis=1)\n",
    "        return spd\n",
    "\n",
    "    def create_subspace_preference_vectors(self):\n",
    "        \"\"\"For each point and attribute we get a weight (thus a vector for multiple attributes),\n",
    "         where the weight is 1 if the variance along the axis is large than delta, else the weight is kappa\n",
    "        :return: numpy ndarray: rows=data points, columns= weight in corresponding attribute\n",
    "        \"\"\"\n",
    "        # Define weight vector: has the same dimensionaliy as the attributes variances & consists of 1's and kappas\n",
    "        weight_vec = np.ones_like(self.attribute_variances, dtype=float)\n",
    "        # set weights whose attributes are smaller than delta to kappa\n",
    "        weight_vec[self.attribute_variances <= self.delta] = self.kappa\n",
    "        return weight_vec\n",
    "\n",
    "    \n",
    "    def create_preference_weighted_similarity_matrix(self):\n",
    "        \"\"\"Calcuate sparse similarity matrix of all points in the data set\n",
    "        :return: numpy ndarray: symmetrical distance matrix using the weighted similarity measure\n",
    "        \"\"\"\n",
    "        # We only need to compare the distances to points within the epsilon shell (to determine if a point is a core point)\n",
    "        # Since the subspace scaling factor kappa is >>1 (and not <1), no distances to other points will be needed for \n",
    "        # the core point evaluation\n",
    "\n",
    "        # get points in epsilon shell: attententio point itself is not in neigh_ind list\n",
    "        _, neigh_ind = self.neigbors_clf.radius_neighbors(radius=self.epsilon)\n",
    "        row, col, pwsim = [], [], []\n",
    "        for i, ith_neigh_ind in enumerate(neigh_ind):\n",
    "            # Calculate preference weighted similarity measure with point and neighbors in eps shell\n",
    "            sq_diffs = np.square(self.data[ith_neigh_ind,:] - self.data[i,:])\n",
    "            sum_weighted_sq_diffs = np.inner(self.subspace_preference_vectors[i,:], sq_diffs)\n",
    "            pwsim_ith = np.sqrt(sum_weighted_sq_diffs)\n",
    "            \n",
    "            # Info for sparse matrix\n",
    "            pwsim.extend(pwsim_ith.tolist())      # Data\n",
    "            row.extend([i]*(pwsim_ith.shape[0]))  # ith Row \n",
    "            col.extend(ith_neigh_ind.tolist())    # column info\n",
    "\n",
    "        # Construct sparse matrix with data, row, and column info\n",
    "        A = csr_matrix((pwsim, (row, col)), shape=(self.nb_points, self.nb_points))\n",
    "        # Create symmetric version: take the elementwise maximum of A and its transpose A.T\n",
    "        transpose_is_bigger = A.T>A\n",
    "        A = A - A.multiply(transpose_is_bigger) + (A.T).multiply(transpose_is_bigger)\n",
    "        \n",
    "        return A\n",
    "    \n",
    "\n",
    "    def create_preference_weighted_epsilon_neighbourhood(self):\n",
    "        \"\"\"New neighborhood definition under the weighted similarity measure\n",
    "        :return: List of numpy 1D arrays containing indices of points in the preference weighted eps. nbh\n",
    "        \"\"\"\n",
    "        \n",
    "        A = self.pref_weighted_similarity_measures   # distances matrix\n",
    "        A[A>self.epsilon] = 0   # set distances greater than epsilon to 0\n",
    "        A.eliminate_zeros()     # then remove these entries from matrix\n",
    "        # For each entry in data get neighbor indices with preference weighted distance less than epsilon\n",
    "        weighted_eps_neighbh = np.split(A.indices, A.indptr)[1:-1]  \n",
    "\n",
    "        return weighted_eps_neighbh\n",
    "\n",
    "    def create_preference_weighted_core_points(self):\n",
    "        \"\"\" For each point in the data set test if the conditions 1&2 to a preference weighted core point are satisfied:\n",
    "            1. Subspace dimensionality is smaller or equal than lambda\n",
    "            2. The number of preference weighted epsilon neighbors is larger than mu\n",
    "        :return: numpy 1D array: boolian type array (True: is core point)\n",
    "        \"\"\"\n",
    "        is_core_point = np.array([(len(pwn)+1>=self.mu) & (spd<=self.lamb)   # +1 because point itself is not taken into account in radius neighbor query\n",
    "                                  for pwn, spd in zip(self.preference_weighted_neighbourhoods,\n",
    "                                                      self.subspace_preference_dimensionalities)])\n",
    "        return is_core_point\n",
    "\n",
    "    def fit(self):\n",
    "        \"\"\"Run clustering algorithm\n",
    "        :return: cluster labels for each point\n",
    "        \"\"\"\n",
    "        labels_dict = {}          # Stores the processed data points and respective label info\n",
    "        current_label_id = -1     # Cluster label id\n",
    "        for point_idx in range(self.nb_points):\n",
    "            if point_idx not in labels_dict:  # if point has not already been visited\n",
    "                if self.preference_weighted_core_points[point_idx]:  # Is it a core point?\n",
    "                    current_label_id += 1  # new cluster -> increment counter\n",
    "                    # Go through the neighbors and search there for other cluster members\n",
    "                    cluster_member_search_list = self.preference_weighted_neighbourhoods[point_idx].tolist()\n",
    "                    while cluster_member_search_list:\n",
    "                        cluster_candidate = cluster_member_search_list.pop()\n",
    "                        possible_cluster_candidates = {cluster_candidate}\n",
    "                        if self.preference_weighted_core_points[cluster_candidate]:  # If candidate is a core point\n",
    "                            # go through each point in the neighborhood of the cluster_candidate\n",
    "                            for candidate_neighbors in self.preference_weighted_neighbourhoods[cluster_candidate]:\n",
    "                                has_pdim = self.subspace_preference_dimensionalities[cluster_candidate]<=self.lamb\n",
    "                                is_unclassified = candidate_neighbors not in labels_dict\n",
    "                                if has_pdim and is_unclassified:\n",
    "                                    possible_cluster_candidates.add(candidate_neighbors)\n",
    "\n",
    "                        for new_point in possible_cluster_candidates:\n",
    "                            if new_point not in labels_dict:\n",
    "                                if new_point != cluster_candidate:   # should not add the same point again\n",
    "                                    cluster_member_search_list.append(new_point)\n",
    "                                labels_dict[new_point] = current_label_id  # Point gets corresponding cluster id\n",
    "                            elif labels_dict[new_point] == -1:  # if the point is currently classified as noise\n",
    "                                labels_dict[new_point] = current_label_id\n",
    "\n",
    "                else:\n",
    "                    labels_dict[point_idx] = -1  # Save as noise point\n",
    "        # fill the labels_ array\n",
    "        for p_idx, label_id in labels_dict.items():\n",
    "            self.labels_[p_idx] = label_id\n",
    "\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
